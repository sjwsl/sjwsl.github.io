---
# layout: post
title: MIT 6.824 Lab 1
categories: 
    - Lab Report
tag:
    - "6.824"
    - Golang
    - Distributed Systems
    - MapReduce
---

In this lab you'll build a MapReduce system. You'll implement a worker process that calls application Map and Reduce functions and handles reading and writing files, and a master process that hands out tasks to workers and copes with failed workers.

[Lab HomePage](http://nil.csail.mit.edu/6.824/2020/labs/lab-mr.html)

## Goal

用 Go 语言实现一个简单的 MapReduce 框架，master 和 worker 之间用 RPC 通信。``main`` 目录下包含了 master 和 worker 的入口，这部分不需要改动。我们要修改的文件是 ``mr/master.go``、``mr/worker.go``、`mr/rpc.go`，分别对应了 master 和 worker 的实现，以及所需 RPC 的定义。

## Overview

MapReduce 本质上是 kv 到 kv 的映射过程。以 WordCount 为例，Map 的过程我们可以理解为 `(filename, content)` 映射到若干 `(word, count)`，最简单的映射规则是：对于每一个出现在 `content` 中的 `word`，映射到 `(word, 1)`。而 Reduce 是 `(word, list(count))` 映射到 `(word, total_count)`，映射的规则就是 `total_count = sum(list(count))`。`mrsequential.go` 中给出了单机顺序 MapReduce 的实现。

实际我们并不需要关心映射规则，实验中 `mapf` 和 `reducef` 是以接口的方式提供的。我们只要关注函数的签名分别是 `(string, string) -> list(string, string)` 和 ``(string, list(string)) -> string``，``reducef`` 这里的 `string` 实际上就是 `(string, string)` 的格式化输出形式。

## RPC

RPC 只需要 worker 到 master 单向调用，我这里设计了四个 RPC 分别表示 map 和 reduce 两种任务的申请和完成。另外 worker 需要从 master 得到 map 和 reduce 任务的数量，也需要一个 RPC。有些时候 Args 和 Reply 是不需要的，这里为了方便设计两个空结构体。全部 RPC 如下。

```go
type NilArgs struct {
}

type NilReply struct {
}

type GetNReply struct {
	NMap    int
	NReduce int
}

type MapRequestReply struct {
	MapTaskID int
	Filename  string
	Success   bool
	Retry     bool
}

type ReduceRequestReply struct {
	ReduceTaskID int
	Success      bool
	Retry        bool
}

type MapTaskDoneArgs struct {
	MapTaskID int
}

type ReduceTaskDoneArgs struct {
	ReduceTaskID int
}
```

注意 Go 的结构成员变量首字母大写视为 public，否则视为 private。

## Worker

worker 的逻辑很简单，首先初始化所需变量，一个无限循环请求 map task 直到 map task 全部完成，然后一个无限循环请求 reduce task，直到 reduce task 全部完成，然后结束。

请求 map 的循环如下，请求 reduce 基本一模一样

```go
for {
    mapRequestReply := MapRequestReply{}
    call("Master.MapRequest", &NilArgs{}, &mapRequestReply)
    if !mapRequestReply.Success {
        if mapRequestReply.Retry {
            fmt.Println("waiting for map")
            time.Sleep(time.Second)
            continue
        } else {
            fmt.Println("map all done")
            break
        }
    }
    mapTaskID := mapRequestReply.MapTaskID
    fmt.Printf("map %v running\n", mapTaskID)
    
    // ...

    fmt.Printf("map %v done\n", mapTaskID)
    call("Master.MapTaskDone", &MapTaskDoneArgs{mapTaskID}, &NilReply{})
}
```

map task 的数量就是 txt 文件的数量，而 reduce task 的数量是给定的 10，我们最后的输出文件数是 reduce task 的数量。

每个 map worker 拿到一个 txt 文件，把 `(word, count)` 输出到 `mr-out-{mapTaskID}-{reduceTaskID}` 中，这里的 `reduceTaskID` 通过 `hash(word)%reduceTaskN` 计算。也就是我们会有 `mapTaskN*reduceTaskN` 个中间文件。

每个 reduce worker 根据自己的 `reduceTaskID` 拿到 mapTaskN 个中间文件，输出到 `mr-out-{reduceTaskID}` 。

这种设计保证了文件读写不需要加锁，reduce worker 在发送 done 之前可以把自己负责的中间文件删除。

## Master

master 用一个结构体存储状态信息和对应的锁。

```go
type TaskStateType int

const (
	TaskStateReady   TaskStateType = 0
	TaskStateRunning TaskStateType = 1
	TaskStateDone    TaskStateType = 2
)

type Master struct {
	files                []string
	nMap                 int
	nReduce              int
	nMapDone             int
	nReduceDone          int
	mapTaskState         []TaskStateType
	reduceTaskState      []TaskStateType
	nMapDoneMutex        sync.RWMutex
	nReduceDoneMutex     sync.RWMutex
	mapTaskStateMutex    sync.Mutex
	reduceTaskStateMutex sync.Mutex
}
```

master 需要提供各种 RPC 接口，以 MapRequest 为例

1. 如果 nMapDone 等于 nMap，表明 map 已经全部结束，请求失败且不用重试
2. 如果 nMapDone 小于 nMap，但找不到一个 `Ready` 的任务，表明有任务在 `Running` 状态，请求失败且需要重试，防止之后 `Running` 的任务失败
3. 如果找到 Ready 状态的任务，请求成功，返回 `mapTaskID`，标记任务为 `running`，实验要求最多等待 10 秒，这里设计一个定时函数，10 秒后将任务标记回 `Ready`

不加锁的版本

```go
func (m *Master) MapRequest(args *NilArgs, reply *MapRequestReply) error {
	if m.nMapDone == m.nMap {
		reply.Success = false
		reply.Retry = false
		return nil
	}
	for i := 0; i < m.nMap; i++ {
		if m.mapTaskState[i] == TaskStateReady {
			fmt.Printf("map %v running\n", i)
			m.mapTaskState[i] = TaskStateRunning
			reply.MapTaskID = i
			reply.Filename = m.files[i]
			reply.Success = true
			_ = time.AfterFunc(10*time.Second, func() {
				if m.mapTaskState[i] == TaskStateRunning {
					fmt.Printf("map %v crash\n", i)
					m.mapTaskState[i] = TaskStateReady
				}
			})
			return nil
		}
	}
	reply.Success = false
	reply.Retry = true

	return nil
}
```

`MapTaskDone` 的实现很简单，这里同样给出不加锁的版本

```go
func (m *Master) MapTaskDone(args *MapTaskDoneArgs, reply *NilReply) error {
	mapTaskID := args.MapTaskID
	fmt.Printf("map %v done\n", mapTaskID)
	m.mapTaskState[mapTaskID] = TaskStateDone
	m.nMapDone++

	return nil
}
```

两个对应的 reduce RPC 基本一模一样，实现完之后可以自己测试单 worker 的结果是否正确的。然后把所有共享变量加锁，就能通过测试了。

## Result

注意测试的时候要开 `--race`，因为就算加锁出错也可能通过测试，要保证检测不出 data race，另外一定要多跑几遍没错才可以。

```
*** Starting wc test.
2020/07/26 17:08:45 rpc.Register: method "Done" has 1 input parameters; needs exactly three
--- wc test: PASS
*** Starting indexer test.
2020/07/26 17:08:55 rpc.Register: method "Done" has 1 input parameters; needs exactly three
--- indexer test: PASS
*** Starting map parallelism test.
2020/07/26 17:09:00 rpc.Register: method "Done" has 1 input parameters; needs exactly three
--- map parallelism test: PASS
*** Starting reduce parallelism test.
2020/07/26 17:09:08 rpc.Register: method "Done" has 1 input parameters; needs exactly three
--- reduce parallelism test: PASS
*** Starting crash test.
2020/07/26 17:09:18 rpc.Register: method "Done" has 1 input parameters; needs exactly three
--- crash test: PASS
*** PASSED ALL TESTS
```

## More

Hint 中提到 `To ensure that nobody observes partially written files in the presence of crashes, the MapReduce paper mentions the trick of using a temporary file and atomically renaming it once it is completely written.` 

当时没按照这个做也通过了测试。考虑真实环境中如果 map worker 10 秒还没有完成任务，master 会把他的任务重新分配。虽然重新分配的另一个 map worker 会首先删除中间文件，但可能在写的过程中上一个 worker 也在写这个文件，这时就会发生错误。而存了 tmp 之后，就算之前的 worker 完成后覆盖掉新的 worker 写的中间文件，但 reduce 观察到的结果还是正确的。

