I"<p>Raft</p>

<p>像 GFS、VMware FT 这样的分布式系统使用 primary/backup 原语来实现容错性，他们的共同点是都依赖一个 Master 节点来决定谁是当前的 primary。这种系统的致命问题是仍然存在单点错误，如果 Master 挂了，服务也就挂了。我们现在考虑如果没有 Master 节点，能否保证分布式系统的一致性和容错性。我们要面对的主要问题是 Split Brain。</p>

<h2 id="split-brain">Split Brain</h2>

<p><strong>脑裂（split brain）</strong> 或者<strong>网络分裂（network partition）</strong>是分布式系统中很严重的问题，根本原因是计算机无法区分停止工作和网络错误，也就是无法连接到某台计算机并不说明这台计算机不在工作。</p>

<p>考虑最简单的情况，两个 server 组成的分布式锁系统，如果没有 Master 这样的节点，我们会发现这个系统是完全没有意义的。因为客户端请求有两种选择：</p>
<ul>
  <li>只需要一台服务器响应：如果出现了脑裂， C1 只能与 S1 通信，C2 只能与 S2 通信，两个客户端都会认为另一个 server 挂了，从而成功获得锁。</li>
  <li>需要两台服务器响应：这样虽然可以正常工作，但完全失去了分布式系统的意义，容错性甚至不如单一服务器（只要任何一个服务器挂了服务就不可用）。</li>
</ul>

<p>在上世纪80年代之前，要解决这个问题有这几种方法</p>
<ul>
  <li>构建一个完全不可能出现故障的网络</li>
  <li>人工解决问题，当一个服务器不响应时，让运维人员查看它的状况。</li>
  <li>引入 Master 节点，实际上和人工解决一样</li>
</ul>

<p>但这些方法显然代价太大，实际上哪怕网络会出现故障，也可以实现自动完成故障切换的系统。</p>

<h2 id="majority-vote">Majority Vote</h2>

<p><strong>过半票决（majority vote）</strong>是用于构建 Raft 的基本概念，基本思想是，任何时候完成任何操作都必须凑够<strong>大于一半</strong>的服务器批准该操作。这要求服务器的数量是奇数，因为偶数的服务器在网络分裂时可能出现完全相同的两部分。</p>

<p>如果一个过半票决的系统有 $2f+1$ 个服务器，那么最多 $f$ 个服务器故障的情况下，系统仍然可以正常工作。</p>

<p>过半票决有两个最关键的性质</p>
<ul>
  <li>最多只有一个网络分区会有过半的服务器</li>
  <li>任意两组过半的服务器，至少有一个重叠，也就是说<strong>每次批准操作的计算机中至少有一个存储了上一次操作的结果</strong></li>
</ul>

<p>过半票决这种思想是在 1990 年左右提出的，之后基于这种思想产生了两种系统 Paxos 和 View-Stamped Replication</p>

<h2 id="raft">Raft</h2>

<h3 id="软件层面">软件层面</h3>

<p>一个 Raft 集群的每个副本包括应用层和 Raft 层，Raft 以 library 的形式为应用层提供服务。</p>

<p>应用层通常都有状态，例如一个 k-v 数据库的状态就是 k-v table。应用层通过调用 Raft 层提供的函数，传递自己的状态并接受返回的信息，从而实现多副本状态的统一。</p>

<p>Raft 层也有状态，最重要的状态就是 log。理想状态下多副本应用层和 Raft 层的状态是完全相同的，如果出现故障，Raft 层可以保证应用层状态的一致性。</p>

<p><img src="/assets/images/raft1.png" alt="" /></p>

<p>对客户端来说，Raft 集群提供的服务和单点服务没有区别。客户端会把请求发送给 Raft 集群中 Leader 节点副本对应的应用层。这里的请求就是 Put/Get 这种应用级别的请求，客户端并不知道 Raft 的存在。</p>

<p>当应用层收到客户端请求时，Raft 并不会立刻响应，而只是会把请求向下发送到 Raft 层。之后，各个副本的 Raft 层相互交互。直到过半副本的 Raft 层都有了 log entry 的拷贝，Leader 节点的 Raft 层会发送一个通知（ApplyMsg）给应用层，这称为 commit。应用层收到通知后才会执行这个请求，改变自己的状态，然后返回给客户端。</p>

<p>当 log entry 被 commit 后，Leader 节点的 Raft 层会通知其他节点的 Raft 层，它们收到通知后也会向自己的应用层发送 ApplyMsg。最终，如果每个节点 Raft 层发送 ApplyMsg 的内容和顺序是完全相同的，应用层的状态也会完全相同。</p>

<p><img src="/assets/images/raft2.png" alt="" /></p>

<h3 id="raft-paper">Raft Paper</h3>

<p>Raft 是一种管理复制日志的<strong>共识算法（consensus algorithm）</strong>，它提供了和 Paxos 相同的功能，但是<strong>更好理解</strong>并且<strong>更容易构建真实系统</strong>。</p>

<h4 id="特点">特点</h4>
<ul>
  <li><strong>强领导者：</strong>Raft 强调了 Leader 的作用。尽管已经证明分布式系统能在无 Leader 的情况下达成共识，Paxos 的论文中也确实是这么做的。但是 Raft 要求只有 Leader 才能接受客户端请求、log entry 只能从 Leader 单向发送到其他服务器，这样使 Raft 算法更加易于理解。</li>
  <li><strong>领导选举：</strong>Raft 使用随机计时器来避免 Split Vote，同样是为了可理解性。</li>
  <li><strong>成员调整：</strong>Raft 使用一种新的 joint consensus 方法来处理集群成员变化。</li>
</ul>

<h4 id="复制状态机">复制状态机</h4>

<p>共识算法是在<strong>复制状态机（replicated state machine）</strong>的背景下提出的，复制状态机的结构如图所示</p>

<p><img src="/assets/images/raft3.png" alt="" /></p>

<p>每一个服务器存储了包含一系列指令的 log，共识算法保证每一个服务器的日志<strong>最终</strong>都以相同的顺序包含相同的指令。如果状态机是确定的，相同的执行序列会生成相同的状态。</p>

<p>一般来说，一致性算法有以下特性：</p>
<ul>
  <li><strong>安全性：</strong>可能停止服务，但不会返回错误的结果。</li>
  <li><strong>可用性：</strong>只要集群中有大多数节点可运行并且能够相互通信，服务就是可用的。</li>
  <li><strong>不依赖时钟：</strong>时钟错误或者极端的消息延迟只有在最坏情况下才会导致可用性问题。</li>
  <li>通常情况下，一条请求可以在大部分节点响应后完成，小部分慢的节点不影响系统延迟。</li>
</ul>

<h4 id="paxos-的问题">Paxos 的问题</h4>

<ul>
  <li>难以理解</li>
  <li>难以基于 Paxos 构建真实系统：Paxos 解决的实际上是比 Raft 小很多的问题，论文主要描述了如何对一条日志达成共识。作者提到了 multi-Paxos 的多种实现方式，然而缺乏细节。Paxos 没有 Leader，在一条决策的时候这是很有意义的。然而真实系统要处理无限的 log 序列，如果有一个 Leader，很多工作会更简单快速。</li>
</ul>

<h4 id="raft-算法">Raft 算法</h4>

<p>Raft 通过选举 Leader，赋予它全部管理复制 log 的责任来实现一致性。Leader 从客户端接受请求，写入自己的 log，然后复制到其他服务器上。当大多数服务器写入这条 log entry 后，Leader 通知所有服务器把命令执行到状态机上。</p>

<p>Raft 将一致性问题分解为三个相对独立的子问题</p>
<ul>
  <li><strong>Leader Election：</strong> Leader 可能会宕机，可能失去连接，这时系统会选举新的 Leader。</li>
  <li><strong>Log Replication：</strong> Leader 会强制要求其他节点的日志和自己相同。</li>
  <li><strong>Safety：</strong>Raft 的安全性主要来自下图的<strong>状态机安全（state machine safety）</strong></li>
</ul>

<p><img src="/assets/images/raft4.png" alt="" /></p>
:ET